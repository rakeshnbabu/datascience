{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Machine learning types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Figure%201.6.jpg\" align=\"middle\" alt=\"Figure 1.6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Machine learning applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Figure%201.7.png\" style=\"width:70%;height:70%\" align=\"middle\" alt=\"Figure 1.7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Choosing the right technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ml_map.png\" style=\"width:90%;height:90%\" align=\"middle\" alt=\"ml_map\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Basic hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I'm using a very small dataset of <b>student test scores</b> and <b>the amount of hours they studied.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.8.png\" style=\"width:50%;height:50%\" align=\"middle\" alt=\"Figure 1.8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intuitively, we know that there must be a relationship right? The more you study, the better your test scores should be. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We're going to use <b><i>linear regression</i></b> to prove this relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. steps we are going to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data\n",
    "2. Initialization of the parameters\n",
    "3. Define Linear Equation\n",
    "4. Define and understand <b>Sum of Squared Error value</b> and <b>equation</b>\n",
    "5. Calculate <b>Gradient Descent</b> in order to get the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. linear Regression using Gradient Descent based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%20%201.9.gif\" style=\"width:100%;height:100%\" align=\"middle\" alt=\"Figure 1.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. linear Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.10.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.10\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. sum of squared error value and equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.11.png\" style=\"width:50%;height:50%\" align=\"middle\" alt=\"Figure 1.11\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. sum of squared distance equation in statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.12.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.12\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. sum of squared distances formula (to calculate our error) linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.13.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.13\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.14.png\" style=\"width:80%;height:80%\" align=\"middle\" alt=\"Figure 1.14\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.15.png\" style=\"width:60%;height:60%\" align=\"middle\" alt=\"Figure 1.15\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.16.png\" style=\"width:70%;height:70%\" align=\"middle\" alt=\"Figure 1.16\">\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. partial derivative with respect to b and m (to perform gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.17.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.17\">\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, MSE = 5565.107834483211\n",
      "Running...\n",
      "After 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, MSE = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "# here we are calculating the sum of squared error by using the equation which we have seen.\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # Here we are coding up out partial derivatives equations and\n",
    "        # generate the updated value for m and b to get the local minima\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    # we are multiplying the b_gradient and m_gradient with learningrate\n",
    "    # so it is important to choose ideal learning rate if we make it to high then our model learn nothing\n",
    "    # if we make it to small then our training is to slow and there are the chances of over fitting\n",
    "    # so learning rate is important hyper parameter.\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        # we are using step_gradient function to calculate the actual partial derivatives for error function\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    # Step 1 : Read data\n",
    "\n",
    "    # genfromtext is used to read out data from data.csv file.\n",
    "    points = genfromtxt(\"../data/data1.csv\", delimiter=\",\")\n",
    "\n",
    "    # Step2 : Define certain hyperparameters\n",
    "\n",
    "    # how fast our model will converge means how fast we will get the line of best fit.\n",
    "    # Converge means how fast our ML model get the optimal line of best fit.\n",
    "    learning_rate = 0.0001\n",
    "    # Here we need to draw the line which is best fit for our data.\n",
    "    # so we are using y = mx + b ( x and y are points; m is slop; b is the y intercept)\n",
    "    # for initial y-intercept guess\n",
    "    initial_b = 0\n",
    "    # initial slope guess\n",
    "    initial_m = 0\n",
    "    # How much do you want to train the model?\n",
    "    # Here data set is small so we iterate this model for 1000 times.\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    # Step 3 - print the values of b, m and all function which calculate gradient descent and errors\n",
    "    # Here we are printing the initial values of b, m and error.\n",
    "    # As well as there is the function compute_error_for_line_given_points()\n",
    "    # which compute the errors for given point\n",
    "    print (\"Starting gradient descent at b = {0}, m = {1}, MSE = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print (\"Running...\")\n",
    "\n",
    "    # By using this gradient_descent_runner() function we will actually calculate gradient descent\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "\n",
    "    # Here we are printing the values of b, m and error after getting the line of best fit for the given dataset.\n",
    "    print (\"After {0} iterations b = {1}, m = {2}, MSE = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5) Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data1.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 1)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,0]\n",
    "print(X.shape)\n",
    "X = X.values.reshape(-1,1)\n",
    "print(X.shape)\n",
    "y = data.iloc[:,1]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(X,y)\n",
    "y2 = lr2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  110.257383466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE = ', mean_squared_error(y, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a0b487128>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkEvIpJx/wWEMb54kle8PQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13a53358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, y2, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your turn .. fit Stochastic-Gradient-Descent using Scikit-learn!\n",
    "# SGDRegressor\n",
    "# Measure performance\n",
    "# Validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6) Boston housing dataset using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe the dataset\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your turn!\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. split data into training and test set (75-25 or 80-20)\n",
    "2. fit Linear Regression : median value vs RM (average number of rooms)\n",
    "3. fit Linear Regression : median value vs all features \n",
    "4. measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
